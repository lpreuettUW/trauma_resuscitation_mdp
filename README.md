# Trauma Resuscitation Markov Decision Process (MDP)
This repository accompanies our paper â€œTBDâ€ and provides all code needed to reproduce the development and analysis of a Markov Decision Process (MDP) for trauma resuscitation. The project models early clinical decision-making for trauma patients using real-world electronic health record (EHR) data and formalizes the problem as a sequential decision process to support reinforcement learning research in critical care.

We provide tools for data processing, MDP construction, and baseline policy evaluation, along with scripts to reproduce the figures and results presented in the paper.

# ğŸ“ Repository Structure
```
trauma_resuscitation_mdp/
â”‚â”€â”€ agents/
â”‚   â”œâ”€â”€ abstract_agent.py           # Abstract class for all agents
â”‚   â”œâ”€â”€ abstract_batch_agent.py     # Abstract class for batch learning agents
â”‚   â”œâ”€â”€ abstract_sequence_agent.py  # Abstract class for sequence agents (NOTE: placeholder)
â”‚   â”œâ”€â”€ d3qn.py                     # Dueling Double Deep Q-Network agent (D3QN)
â”‚   â”œâ”€â”€ implicit_q_learning.py      # Implicit Q-Learning agent (IQL)
â”‚   â”œâ”€â”€ no_action_agent.py          # Agent that takes no action
â”‚   â”œâ”€â”€ random_action_agent.py      # Agent that takes random actions
â”‚â”€â”€ datasets/
â”‚   â”œâ”€â”€ trauma_icu_resuscitation/
â”‚   â”‚   â”œâ”€â”€ extract_trajectories.py                 # Clean, Preprocess, and Extract Trajectories
â”‚   â”‚   â”œâ”€â”€ _data_manager.py/                       # Loads Trauma ICU Resuscitation data
â”‚   â”‚   â”œâ”€â”€ _trajectory_extraction_utilities.py     # Utilities for extracting trajectories
â”‚   â”‚   â”œâ”€â”€ stratified_splits/                      # Stratified splits for train/val/test
â”‚â”€â”€ mdp/
â”‚   â”‚â”€â”€ policies/
â”‚   â”‚   â”œâ”€â”€ dueling_dqn.py                  # Policy for D3QN agent
â”‚   â”‚   â”œâ”€â”€ next_best_action_policy.py      # Policy for IQL agent
â”‚   â”‚â”€â”€ trauma_icu_resuscitation/
â”‚   â”‚   â”œâ”€â”€ action_spaces/
â”‚   â”‚   â”‚   â”œâ”€â”€ binary.py                   # Binary action space
â”‚   â”‚   â”‚   â”œâ”€â”€ discrete.py                 # Discrete action space (NOTE: this is the final action space)
â”‚   â”‚   â”œâ”€â”€ state_spaces/
â”‚   â”‚   â”‚   â”œâ”€â”€ discrete.py                 # Discrete state space
â”‚   â”‚â”€â”€ action.py
â”‚â”€â”€ notebooks/
â”‚   â”œâ”€â”€ cohort_eda.ipynb                    # Exploratory Data Analysis (EDA) of the cohort data
â”‚   â”œâ”€â”€ create_splits.ipynb                 # Create stratified splits for train/val/test
â”‚   â”œâ”€â”€ extracted_trajectory_eda.ipynb      # EDA of the extracted trajectories
â”‚   â”œâ”€â”€ interventions_eda.ipynb             # EDA of the interventions
â”‚   â”œâ”€â”€ ivf_returns.ipynb                   # Analyze returns for MDP on our dataset
â”‚   â”œâ”€â”€ ope_inspection.ipynb                # Analyze the OPE results
â”‚   â”œâ”€â”€ policy_inspection.ipynb             # Analyze the behavior and learned policies
â”‚   â”œâ”€â”€ vitals_and_labs_eda.ipynb           # EDA of the vitals and labs
â”‚â”€â”€ ope/
â”‚   â”œâ”€â”€ abstract_ope_method.py              # Abstract class for OPE methods
â”‚   â”œâ”€â”€ behavior_policy_value.py            # Compute behavior policy value estimate
â”‚   â”œâ”€â”€ fqe.py                              # Fitted Q-Evaluation (FQE) method
â”‚   â”œâ”€â”€ magic.py                            # Model and Guided Importance sampling Combining (MAGIC) method
â”‚â”€â”€ utilities/
â”‚   â”œâ”€â”€ device_manager.py                   # Torch device manager for GPU/CPU usage
â”‚   â”œâ”€â”€ implicit_qlearning_dataset.py       # Dataset for D3QN and IQL
â”‚   â”œâ”€â”€ ope_trajectory_dataset.py           # Dataset for OPE
â”‚   â”œâ”€â”€ sequence_agent_interface.py         # Placeholder
â”‚   â”œâ”€â”€ trauma_icu_resuscitation_funcs.py   # Utilities for the Trauma ICU Resuscitation dataset
â”‚â”€â”€ do_ope.py       # Run OPE
â”‚â”€â”€ train_d3qn.py   # Train D3QN agent
â”‚â”€â”€ train_iql.py    # Train IQL agent
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ LICENSE
```
# ğŸ—ï¸ Installation (Using Conda)
## 1ï¸âƒ£ Clone the repository
```
git clone https://github.com/lpreuettUW/trauma_resuscitation_mdp.git
cd trauma_resuscitation_mdp
```
## 2ï¸âƒ£ Create and activate a conda environment
Ensure you have Codna installed, then create an environment with Python 3.10:
```
conda create -n trauma_resuscitation_mdp python=3.10
conda activate trauma_resuscitation_mdp
```
## 3ï¸âƒ£ Install dependencies
```
pip install -r requirements.txt
```
## 4ï¸âƒ£ Download the Trauma ICU Resuscitation dataset
Download the dataset from TBD.
## 5ï¸âƒ£ Update Paths
Replace all instances of <path_to_dataset> in the codebase with the path to your downloaded dataset.<br>
Replace all instances of <path_to_repo> with the path to your cloned repository.<br>
Finally, replace all instances of <path_to_mlruns> with your desired directory for storing MLflow experiments.
## (Optional) 6ï¸âƒ£ Create splits
If you want to create your own stratified splits for train/val/test, remove the contents of `datasets/trauma_icu_resuscitation/stratified_splits` before running the `create_splits.ipynb` notebook. 
# ğŸš€ Running Experiments
## 1ï¸âƒ£ Train D3QN
To train the D3QN agent, run the following command:
```
python train_d3qn.py
```
## 2ï¸âƒ£ Train IQL
To train the IQL agent, run the following command:
```
python train_iql.py
```
## 3ï¸âƒ£ Run OPE
To evaluate a given agent using MAGIC, run the following command:
```
python do_ope.py --run_type magic --action_type discrete --agent_type <agent_type> --agent_runid <mlflow_run_id_of_agent>
```
